{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/juansegundohevia/Documents/repos/TexTract\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import bleu_score\n",
    "import pickle as pkl\n",
    "import os\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "import pix2tex.cli as p2t\n",
    "\n",
    "\n",
    "def detokenize(tokens, tokenizer):\n",
    "    toks = [tokenizer.convert_ids_to_tokens(tok) for tok in tokens]\n",
    "    for b in range(len(toks)):\n",
    "        for i in reversed(range(len(toks[b]))):\n",
    "            if toks[b][i] is None:\n",
    "                toks[b][i] = ''\n",
    "            toks[b][i] = toks[b][i].replace('Ġ', ' ').strip()\n",
    "            if toks[b][i] in (['[BOS]', '[EOS]', '[PAD]']):\n",
    "                del toks[b][i]\n",
    "    return toks\n",
    "\n",
    "def get_tokens(prediction, label):\n",
    "\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"model/dataset/tokenizer.json\")\n",
    "    pred = detokenize(prediction, tokenizer)\n",
    "    truth = detokenize(label, tokenizer)\n",
    "    \n",
    "    return pred, truth\n",
    "#bleus.append(metrics.bleu_score(pred, [alternatives(x) for x in truth]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"notebooks/preds/hw_preds.pkl\", \"rb\") as f:\n",
    "    hw_preds = pkl.load(f)\n",
    "\n",
    "with open(\"notebooks/preds/original_preds.pkl\", \"rb\") as f:\n",
    "    original_preds = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y = A x + A ^ { 2 }\\n',\n",
       " 'B _ { n } ( 1 - x ) = ( - 1 ) ^ { n } B _ { n } ( x )\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get hw labels\n",
    "with open(\"pix2tex/dataset/handwritten/CROHME_math.txt\", \"r\") as f:\n",
    "    hw_labels = f.readlines()\n",
    "\n",
    "hw_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [[\"this\", \"is\", \"a\", \"test\"], [\"this\", \"is\", \"test\"]]\n",
    "candidate = [\"this\", \"is\", \"a\", \"test\"]\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pix2tex.dataset.dataset import Im2LatexDataset\n",
    "\n",
    "with open(\"pix2tex/dataset/handwritten/test_hw.pkl\", \"rb\") as f:\n",
    "    test_hw = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pix2tex.eval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pix2tex as p2t\n",
    "from munch import Munch\n",
    "import yaml\n",
    "from pix2tex import cli as p2t\n",
    "import matplotlib.pyplot as plt\n",
    "from pix2tex.dataset.dataset import Im2LatexDataset\n",
    "from pix2tex.eval import evaluate\n",
    "from torchtext.data import metrics\n",
    "\n",
    "hw_config = Munch({\n",
    "    \"config\": \"settings/handwritten_training.yaml\",\n",
    "    \"checkpoint\": \"../../hw_checkpoints/handwritten_training/handwritten_training_e19_step63.pth\",\n",
    "    \"no_cuda\": True,\n",
    "    \"no_resize\": False\n",
    "})\n",
    "\n",
    "original_config = Munch({\n",
    "    \"config\": \"settings/config.yaml\",\n",
    "    \"checkpoint\": \"model/checkpoints/weights.pth\",\n",
    "    \"no_cuda\": True,\n",
    "    \"no_resize\": False\n",
    "}) \n",
    "hw_model = p2t.LatexOCR(hw_config)\n",
    "original_model = p2t.LatexOCR()\n",
    "# load test set for handwritten files\n",
    "hw_test_set = Im2LatexDataset().load(\"pix2tex/dataset/handwritten/test_v2.pkl\")\n",
    "original_test_set = Im2LatexDataset().load(\"pix2tex/dataset/formulae/test.pkl\")\n",
    "# load yaml files to parse configurations\n",
    "with open(\"pix2tex/model/settings/handwritten_training.yaml\", 'r') as f:\n",
    "    hw_config_yaml = Munch(yaml.safe_load(f))\n",
    "\n",
    "with open(\"pix2tex/model/settings/config.yaml\", 'r') as f:\n",
    "    original_config_yaml = Munch(yaml.safe_load(f))\n",
    "\n",
    "hw_config_yaml.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_config_yaml.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pix2tex.eval' from '/Users/juansegundohevia/Documents/repos/TexTract/pix2tex/eval.py'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pix2tex import eval\n",
    "import importlib\n",
    "importlib.reload(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1083/1083 [12:57<00:00,  1.39it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'truth' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/juansegundohevia/Documents/repos/TexTract/notebooks/05_compute_performance.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juansegundohevia/Documents/repos/TexTract/notebooks/05_compute_performance.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bleu_score, edit_distance, token_accuracy, failure_count, bleus, edit_dists, token_acc \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_everything(hw_model\u001b[39m.\u001b[39;49mmodel, hw_test_set, hw_config_yaml)\n",
      "File \u001b[0;32m~/anaconda3/envs/pix2text/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/repos/TexTract/pix2tex/eval.py:161\u001b[0m, in \u001b[0;36mevaluate_everything\u001b[0;34m(model, dataset, args, num_batches, name)\u001b[0m\n\u001b[1;32m    159\u001b[0m     log[name\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/token_acc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m token_accuracy\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (truth, pred))\n\u001b[1;32m    162\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBLEU: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m bleu_score)\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m bleu_score, edit_distance, token_accuracy, failure_count, bleus, edit_dists, token_acc\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'truth' referenced before assignment"
     ]
    }
   ],
   "source": [
    "bleu_score, edit_distance, token_accuracy, failure_count, bleus, edit_dists, token_acc = eval.evaluate_everything(hw_model.model, hw_test_set, hw_config_yaml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
